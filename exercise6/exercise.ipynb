{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science 2025\n",
    "\n",
    "# Week 6: Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 | Linear regression with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [TED Talks](https://www.kaggle.com/rounakbanik/ted-talks) dataset from Kaggle. Your task is to predict both the ratings and the number of views of a given TED talk. You should focus only on the <span style=\"font-weight: bold\">ted_main</span> table.\n",
    "\n",
    "1. Download the data, extract the following ratings from column <span style=\"font-weight: bold\">ratings</span>: <span style=\"font-weight: bold\">Funny</span>, <span style=\"font-weight: bold\">Confusing</span>, <span style=\"font-weight: bold\">Inspiring</span>. Store these values into respective columns so that they are easier to access. Next, extract the tags from column <span style=\"font-weight: bold\">tags</span>. Count the number of occurrences of each tag and select the top-100 most common tags. Create a binary variable for each of these and include them in your data table, so that you can directly see whether a given tag (among the top-100 tags) is used in a given TED talk or not. The dataset you compose should have dimension (2550, 104), and comprise of the 'views' column, the three columns with counts of \"Funny\", \"Confusing and \"Inspiring\" ratings, and 100 columns which one-hot encode the top-100 most common tag columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('ted_main.csv')\n",
    "\n",
    "def extract_rating(ratings_str, rating_name):\n",
    "    try:\n",
    "        ratings_list = ast.literal_eval(ratings_str)\n",
    "        for rating in ratings_list:\n",
    "            if rating['name'] == rating_name:\n",
    "                return rating['count']\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['Funny'] = df['ratings'].apply(lambda x: extract_rating(x, 'Funny'))\n",
    "df['Confusing'] = df['ratings'].apply(lambda x: extract_rating(x, 'Confusing'))\n",
    "df['Inspiring'] = df['ratings'].apply(lambda x: extract_rating(x, 'Inspiring'))\n",
    "\n",
    "all_tags = []\n",
    "for tags_str in df['tags']:\n",
    "    try:\n",
    "        tags_list = ast.literal_eval(tags_str)\n",
    "        all_tags.extend(tags_list)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "tag_counts = Counter(all_tags)\n",
    "top_100_tags = [tag for tag, count in tag_counts.most_common(100)]\n",
    "\n",
    "for tag in top_100_tags:\n",
    "    df[f'tag_{tag}'] = df['tags'].apply(lambda x: 1 if tag in str(x) else 0)\n",
    "\n",
    "columns_to_keep = ['views', 'Funny', 'Confusing', 'Inspiring'] + [f'tag_{tag}' for tag in top_100_tags]\n",
    "ted_data = df[columns_to_keep].copy()\n",
    "\n",
    "print(f\"Dataset shape: {ted_data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(ted_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct a linear regression model to predict the number of views based on the data in the <span style=\"font-weight: bold\">ted_main</span> table, including the binary variables for the top-100 tags that you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = ted_data.drop('views', axis=1) \n",
    "y = ted_data['views'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr_views = LinearRegression()\n",
    "lr_views.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr_views.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Linear Regression Model for Views Prediction:\")\n",
    "print(f\"Mean Squared Error: {mse:,.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"\\nModel coefficients shape: {lr_views.coef_.shape}\")\n",
    "print(f\"Intercept: {lr_views.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Do the same for the <span style=\"font-weight: bold\">Funny</span>, <span style=\"font-weight: bold\">Confusing</span>, and <span style=\"font-weight: bold\">Inspiring</span> ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "tag_columns = [col for col in ted_data.columns if col.startswith('tag_')]\n",
    "X_tags = ted_data[tag_columns]\n",
    "\n",
    "results = {}\n",
    "\n",
    "y_funny = ted_data['Funny']\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_tags, y_funny, test_size=0.2, random_state=42)\n",
    "lr_funny = LinearRegression()\n",
    "lr_funny.fit(X_train_f, y_train_f)\n",
    "y_pred_f = lr_funny.predict(X_test_f)\n",
    "results['Funny'] = {\n",
    "    'model': lr_funny,\n",
    "    'mse': mean_squared_error(y_test_f, y_pred_f),\n",
    "    'r2': r2_score(y_test_f, y_pred_f)\n",
    "}\n",
    "\n",
    "y_confusing = ted_data['Confusing']\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_tags, y_confusing, test_size=0.2, random_state=42)\n",
    "lr_confusing = LinearRegression()\n",
    "lr_confusing.fit(X_train_c, y_train_c)\n",
    "y_pred_c = lr_confusing.predict(X_test_c)\n",
    "results['Confusing'] = {\n",
    "    'model': lr_confusing,\n",
    "    'mse': mean_squared_error(y_test_c, y_pred_c),\n",
    "    'r2': r2_score(y_test_c, y_pred_c)\n",
    "}\n",
    "\n",
    "y_inspiring = ted_data['Inspiring']\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_tags, y_inspiring, test_size=0.2, random_state=42)\n",
    "lr_inspiring = LinearRegression()\n",
    "lr_inspiring.fit(X_train_i, y_train_i)\n",
    "y_pred_i = lr_inspiring.predict(X_test_i)\n",
    "results['Inspiring'] = {\n",
    "    'model': lr_inspiring,\n",
    "    'mse': mean_squared_error(y_test_i, y_pred_i),\n",
    "    'r2': r2_score(y_test_i, y_pred_i)\n",
    "}\n",
    "\n",
    "print(\"Linear Regression Models for Ratings Prediction:\\n\")\n",
    "for rating_name, metrics in results.items():\n",
    "    print(f\"{rating_name}:\")\n",
    "    print(f\"  MSE: {metrics['mse']:,.2f}\")\n",
    "    print(f\"  R² Score: {metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You will probably notice that most of the tags are not useful in predicting the views and the ratings. You should use some kind of variable selection to prune the set of tags that are included in the model. You can use for example classical p-values or more modern [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) techniques. Which tags are the best predictors of each of the response variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_features = {}\n",
    "\n",
    "print(\"\\n1. Views Prediction:\")\n",
    "X_views = ted_data[tag_columns]\n",
    "y_views = ted_data['views']\n",
    "lasso_views = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_views.fit(X_views, y_views)\n",
    "\n",
    "coef_views = pd.Series(lasso_views.coef_, index=tag_columns)\n",
    "selected_views = coef_views[coef_views != 0].sort_values(ascending=False)\n",
    "selected_features['Views'] = selected_views\n",
    "\n",
    "print(f\"  Best alpha: {lasso_views.alpha_:.4f}\")\n",
    "print(f\"  Number of selected tags: {len(selected_views)}\")\n",
    "print(f\"  Top 10 positive predictors:\")\n",
    "for tag, coef in selected_views.head(10).items():\n",
    "    print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "if len(selected_views[selected_views < 0]) > 0:\n",
    "    print(f\"  Top 5 negative predictors:\")\n",
    "    for tag, coef in selected_views.tail(5).items():\n",
    "        print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "\n",
    "print(\"\\n2. Funny Rating Prediction:\")\n",
    "y_funny = ted_data['Funny']\n",
    "lasso_funny = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_funny.fit(X_tags, y_funny)\n",
    "\n",
    "coef_funny = pd.Series(lasso_funny.coef_, index=tag_columns)\n",
    "selected_funny = coef_funny[coef_funny != 0].sort_values(ascending=False)\n",
    "selected_features['Funny'] = selected_funny\n",
    "\n",
    "print(f\"  Best alpha: {lasso_funny.alpha_:.4f}\")\n",
    "print(f\"  Number of selected tags: {len(selected_funny)}\")\n",
    "print(f\"  Top 10 positive predictors:\")\n",
    "for tag, coef in selected_funny.head(10).items():\n",
    "    print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "if len(selected_funny[selected_funny < 0]) > 0:\n",
    "    print(f\"  Top 5 negative predictors:\")\n",
    "    for tag, coef in selected_funny.tail(5).items():\n",
    "        print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "\n",
    "print(\"\\n3. Confusing Rating Prediction:\")\n",
    "y_confusing = ted_data['Confusing']\n",
    "lasso_confusing = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_confusing.fit(X_tags, y_confusing)\n",
    "\n",
    "coef_confusing = pd.Series(lasso_confusing.coef_, index=tag_columns)\n",
    "selected_confusing = coef_confusing[coef_confusing != 0].sort_values(ascending=False)\n",
    "selected_features['Confusing'] = selected_confusing\n",
    "\n",
    "print(f\"  Best alpha: {lasso_confusing.alpha_:.4f}\")\n",
    "print(f\"  Number of selected tags: {len(selected_confusing)}\")\n",
    "print(f\"  Top 10 positive predictors:\")\n",
    "for tag, coef in selected_confusing.head(10).items():\n",
    "    print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "if len(selected_confusing[selected_confusing < 0]) > 0:\n",
    "    print(f\"  Top 5 negative predictors:\")\n",
    "    for tag, coef in selected_confusing.tail(5).items():\n",
    "        print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "\n",
    "print(\"\\n4. Inspiring Rating Prediction:\")\n",
    "y_inspiring = ted_data['Inspiring']\n",
    "lasso_inspiring = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_inspiring.fit(X_tags, y_inspiring)\n",
    "\n",
    "coef_inspiring = pd.Series(lasso_inspiring.coef_, index=tag_columns)\n",
    "selected_inspiring = coef_inspiring[coef_inspiring != 0].sort_values(ascending=False)\n",
    "selected_features['Inspiring'] = selected_inspiring\n",
    "\n",
    "print(f\"  Best alpha: {lasso_inspiring.alpha_:.4f}\")\n",
    "print(f\"  Number of selected tags: {len(selected_inspiring)}\")\n",
    "print(f\"  Top 10 positive predictors:\")\n",
    "for tag, coef in selected_inspiring.head(10).items():\n",
    "    print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")\n",
    "if len(selected_inspiring[selected_inspiring < 0]) > 0:\n",
    "    print(f\"  Top 5 negative predictors:\")\n",
    "    for tag, coef in selected_inspiring.tail(5).items():\n",
    "        print(f\"    {tag.replace('tag_', '')}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Produce summaries of your results. Could you recommend good tags – or tags to avoid! – for speakers targeting plenty of views and/or certain ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary and Recommendations\n\nBased on the LASSO feature selection results, speakers can use the tags with positive coefficients to increase views and target specific ratings. For high views, use popular topics like \"technology\" or \"science\". For \"Inspiring\" ratings, use tags related to personal stories and social impact. For \"Funny\" ratings, choose entertainment-related tags. Speakers should avoid tags with negative coefficients for their target metrics, as these are associated with lower performance in those areas."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 | Symbol classification (part 2)\n",
    "\n",
    "Note that it is strongly recommended to use Python in this exercise. However, if you can find a suitable AutoML implementation for your favorite language (e.g [here](http://h2o-release.s3.amazonaws.com/h2o/master/3888/docs-website/h2o-docs/automl.html) seems to be one for R) then you are free to use that language as well.\n",
    "\n",
    "Use the preprocessed data from week 3 (you can also produce them using the example solutions of week 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This time train a *random forest classifier* on the data. A random forest is a collection of *decision trees*, which makes it an *ensemble* of classifiers. Each tree uses a random subset of the features to make its prediction. Without tuning any parameters, how is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "try:\n",
    "    X_train_symbols = pd.read_csv('X_train_symbols.csv')\n",
    "    X_test_symbols = pd.read_csv('X_test_symbols.csv')\n",
    "    y_train_symbols = pd.read_csv('y_train_symbols.csv').values.ravel()\n",
    "    y_test_symbols = pd.read_csv('y_test_symbols.csv').values.ravel()\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Training set size: {X_train_symbols.shape}\")\n",
    "    print(f\"Test set size: {X_test_symbols.shape}\")\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    rf_classifier.fit(X_train_symbols, y_train_symbols)\n",
    "    \n",
    "    y_pred = rf_classifier.predict(X_test_symbols)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_symbols, y_pred)\n",
    "    \n",
    "    print(f\"\\nRandom Forest Classifier (default parameters):\")\n",
    "    print(f\"Number of trees: {rf_classifier.n_estimators}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_symbols, y_pred))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The amount of trees to use as a part of the random forest is an example of a hyperparameter, because it is a parameter that is set prior to the learning process. In contrast, a parameter is a value in the model that is learned from the data. Train 20 classifiers, with varying amounts of decision trees starting from 10 up until 200, and plot the test accuracy as a function of the amount of classifiers. Does the accuracy keep increasing? Is more better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_trees_range = range(10, 201, 10)\n",
    "accuracies = []\n",
    "\n",
    "for n_trees in n_trees_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
    "    \n",
    "    rf.fit(X_train_symbols, y_train_symbols)\n",
    "    y_pred = rf.predict(X_test_symbols)\n",
    "    \n",
    "    acc = accuracy_score(y_test_symbols, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Trees: {n_trees:3d} | Test Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_trees_range, accuracies, marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Trees', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Random Forest Test Accuracy vs Number of Trees', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(n_trees_range)\n",
    "plt.ylim([min(accuracies) - 0.01, max(accuracies) + 0.01])\n",
    "\n",
    "best_idx = np.argmax(accuracies)\n",
    "best_n_trees = list(n_trees_range)[best_idx]\n",
    "best_accuracy = accuracies[best_idx]\n",
    "plt.axhline(y=best_accuracy, color='r', linestyle='--', alpha=0.5, label=f'Best: {best_accuracy:.4f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If we had picked the amount of decision trees by taking the value with the best test accuracy from the last plot, we would have *overfit* our hyperparameters to the test data. Can you see why it is a mistake to tune hyperparameters of your model by using the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Why Tuning Hyperparameters on Test Data is a Mistake\n\nTuning hyperparameters using the test set causes the model to overfit to that specific test data, making the performance estimate overly optimistic and not representative of true generalization. The test set should only be used once at the very end for final evaluation, which is why we need a separate validation set for hyperparameter tuning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Reshuffle and resplit the data so that it is divided in 3 parts: training (80%), validation (10%) and test (10%). Repeatedly train a model of your choosing (e.g random forest) on the training data, and evaluate it’s performance on the validation set, while tuning the hyperparameters so that the accuracy on the validation set increases. Then, finally evaluate the performance of your model on the test data. What can you say in terms of the generalization of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all = pd.concat([X_train_symbols, X_test_symbols], axis=0)\n",
    "y_all = np.concatenate([y_train_symbols, y_test_symbols])\n",
    "\n",
    "X_train_new, X_temp, y_train_new, y_temp = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "X_val, X_test_new, y_val, y_test_new = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "n_estimators_options = [50, 100, 150, 200]\n",
    "max_depth_options = [10, 20, 30, None]\n",
    "min_samples_split_options = [2, 5, 10]\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_params = {}\n",
    "tuning_results = []\n",
    "\n",
    "\n",
    "for n_est in n_estimators_options:\n",
    "    for max_d in max_depth_options:\n",
    "        for min_split in min_samples_split_options:\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_est,\n",
    "                max_depth=max_d,\n",
    "                min_samples_split=min_split,\n",
    "                random_state=42\n",
    "            )\n",
    "            rf.fit(X_train_new, y_train_new)\n",
    "            \n",
    "            val_accuracy = accuracy_score(y_val, rf.predict(X_val))\n",
    "            \n",
    "            tuning_results.append({\n",
    "                'n_estimators': n_est,\n",
    "                'max_depth': max_d,\n",
    "                'min_samples_split': min_split,\n",
    "                'val_accuracy': val_accuracy\n",
    "            })\n",
    "            \n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_params = {\n",
    "                    'n_estimators': n_est,\n",
    "                    'max_depth': max_d,\n",
    "                    'min_samples_split': min_split\n",
    "                }\n",
    "\n",
    "print(f\"\\nBest hyperparameters found:\")\n",
    "print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "print(f\"  min_samples_split: {best_params['min_samples_split']}\")\n",
    "print(f\"  Validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "final_model.fit(X_train_new, y_train_new)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_new, final_model.predict(X_train_new))\n",
    "val_accuracy = accuracy_score(y_val, final_model.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_new, final_model.predict(X_test_new))\n",
    "\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"  Training accuracy:   {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"  Validation accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(f\"  Test accuracy:       {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nGeneralization analysis:\")\n",
    "if abs(train_accuracy - test_accuracy) < 0.05:\n",
    "    print(\"  The model generalizes well - training and test accuracies are similar.\")\n",
    "elif train_accuracy > test_accuracy + 0.1:\n",
    "    print(\"  The model shows signs of overfitting - training accuracy is much higher than test accuracy.\")\n",
    "else:\n",
    "    print(\"  The model shows reasonable generalization with slight overfitting.\")\n",
    "    \n",
    "print(f\"  Gap between train and test: {(train_accuracy - test_accuracy)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm = confusion_matrix(y_test_new, final_model.predict(X_test_new))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 | TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of picking a suitable model, evaluating its performance and tuning the hyperparameters is very time consuming. A new idea in machine learning is the concept of automating this by using an optimization algorithm to find the best model in the space of models and their hyperparameters. Have a look at [TPOT](https://github.com/EpistasisLab/tpot), an automated ML solution that finds a good model and a good set of hyperparameters automatically. Try it on this data, it should outperform simple models like the ones we tried easily. Note that running the algorithm might take a while, depending on the strength of your computer. \n",
    "\n",
    "*Note*: In case it is running for too long, try checking if the parameters you are using when calling TPOT are reasonable, i.e. try reducing number of ‘generations’ or ‘population_size’. TPOT uses cross-validation internally, so we don’t need our own validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for your code\n",
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(\n",
    "    generations=5,          \n",
    "    population_size=20,     \n",
    "    cv=5,                    \n",
    "    random_state=42,\n",
    "    verbosity=2,            \n",
    "    n_jobs=-1,              \n",
    "    max_time_mins=10,       \n",
    "    max_eval_time_mins=2    \n",
    ")\n",
    "\n",
    "tpot.fit(X_train_new, y_train_new)\n",
    "\n",
    "test_score = tpot.score(X_test_new, y_test_new)\n",
    "tpot.export('tpot_best_pipeline.py')\n",
    "\n",
    "print(tpot.fitted_pipeline_)\n",
    "\n",
    "if test_score > test_accuracy:\n",
    "    print(\"\\nTPOT found a better model through automated search!\")\n",
    "else:\n",
    "    print(\"\\nThe manual Random Forest performed similarly or better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}